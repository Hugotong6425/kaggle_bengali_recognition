{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T09:47:27.654195Z",
     "start_time": "2020-02-27T09:47:27.633363Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import wandb \n",
    "\n",
    "from apex import amp\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchcontrib\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from hw_grapheme.io.load_data import load_processed_data\n",
    "from hw_grapheme.model_archs.se_resnext import se_resnext50\n",
    "from hw_grapheme.models.train import train_model\n",
    "from hw_grapheme.train_utils.create_dataloader import create_dataloaders_train\n",
    "from hw_grapheme.train_utils.train_test_split import stratified_split_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T09:47:29.576501Z",
     "start_time": "2020-02-27T09:47:28.750795Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done, shape: (50210, 128, 128), (50210,), (50210, 3)\n"
     ]
    }
   ],
   "source": [
    "# load processed data \n",
    "pickle_paths = [\n",
    "    \"../data/processed/size_128/train_data_0.pickle\",\n",
    "#     \"../data/processed/size_128/train_data_1.pickle\",\n",
    "#     \"../data/processed/size_128/train_data_2.pickle\",\n",
    "#     \"../data/processed/size_128/train_data_3.pickle\",\n",
    "]\n",
    "\n",
    "image_data, name_data, label_data = load_processed_data(pickle_paths, image_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T09:47:29.979833Z",
     "start_time": "2020-02-27T09:47:29.579685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "# split train valid set\n",
    "n_splits = 5\n",
    "random_seed = 2020\n",
    "\n",
    "train_idx_list, test_idx_list = stratified_split_kfold(\n",
    "    image_data, label_data, n_splits, random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T09:47:30.000049Z",
     "start_time": "2020-02-27T09:47:29.982648Z"
    }
   },
   "outputs": [],
   "source": [
    "# create data_transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.RandomAffine(degrees=10, scale=(1.0, 1.15)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.0692], [0.2051]),\n",
    "        # transforms.ToPILImage(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.0692], [0.2051])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T09:47:30.068388Z",
     "start_time": "2020-02-27T09:47:30.001714Z"
    }
   },
   "outputs": [],
   "source": [
    "# default training setting\n",
    "num_workers = 6\n",
    "pin_memory = True\n",
    "fold = list(range(n_splits))\n",
    "\n",
    "# customize training setting\n",
    "n_epoch = 120\n",
    "batch_size = 512\n",
    "mixed_precision = True\n",
    "\n",
    "model_arch = se_resnext50\n",
    "model_parameter = {}\n",
    "\n",
    "swa = False\n",
    "\n",
    "optimizer = optim.SGD\n",
    "optimizer_parameter = {\"weight_decay\": 1e-4, \"momentum\": 0.9, \"lr\": 0.2, \"nesterov\": True}\n",
    "\n",
    "# whether to use weighted loss for each class\n",
    "is_weighted_class_loss = True\n",
    "\n",
    "# create lr scheduler\n",
    "epoch_scheduler_func = None\n",
    "epoch_scheduler_func_para = {}\n",
    "error_plateau_scheduler_func = optim.lr_scheduler.ReduceLROnPlateau\n",
    "error_plateau_scheduler_func_para = {\"mode\": \"min\", \"factor\": 0.1, \"patience\": 10, \"verbose\": True, \"min_lr\":1e-3}\n",
    "\n",
    "# prob. of using [\"mixup\", \"cutmix\", \"cross_entropy\"] loss\n",
    "train_loss_prob = [0.5, 0.5, 0.0]\n",
    "mixup_alpha = 0.4  # for mixup/cutmix only\n",
    "\n",
    "# weighting of [root, vowel, consonant]\n",
    "head_weights = [0.5, 0.25, 0.25]\n",
    "\n",
    "wandb_log = True\n",
    "\n",
    "# save dir, set None to not save, need to manual create folders first\n",
    "save_dir = \"../models/all_we_get_20200306/\"\n",
    "# save_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-27T09:57:58.387619Z",
     "start_time": "2020-02-27T09:56:11.617230Z"
    },
    "lines_to_next_cell": 2,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ImportError('/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/amp_C.cpython-37m-x86_64-linux-gnu.so: undefined symbol: _ZN3c1011CPUTensorIdEv')\n",
      "Creating train dataloader...\n",
      "Creating valid dataloader...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://app.wandb.ai/hugotong6425/my-project\" target=\"_blank\">https://app.wandb.ai/hugotong6425/my-project</a><br/>\n",
       "                Run page: <a href=\"https://app.wandb.ai/hugotong6425/my-project/runs/ye2d4o9n\" target=\"_blank\">https://app.wandb.ai/hugotong6425/my-project/runs/ye2d4o9n</a><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retry attempt failed:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 376, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 994, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/connection.py\", line 360, in connect\n",
      "    ssl_context=context,\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 370, in ssl_wrap_socket\n",
      "    return context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/ssl.py\", line 423, in wrap_socket\n",
      "    session=session\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/ssl.py\", line 870, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/ssl.py\", line 1139, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 720, in urlopen\n",
      "    method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/util/retry.py\", line 400, in increment\n",
      "    raise six.reraise(type(error), error, _stacktrace)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/packages/six.py\", line 734, in reraise\n",
      "    raise value.with_traceback(tb)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 672, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 376, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/connectionpool.py\", line 994, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/connection.py\", line 360, in connect\n",
      "    ssl_context=context,\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/urllib3/util/ssl_.py\", line 370, in ssl_wrap_socket\n",
      "    return context.wrap_socket(sock, server_hostname=server_hostname)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/ssl.py\", line 423, in wrap_socket\n",
      "    session=session\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/ssl.py\", line 870, in _create\n",
      "    self.do_handshake()\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/ssl.py\", line 1139, in do_handshake\n",
      "    self._sslobj.do_handshake()\n",
      "urllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/wandb/retry.py\", line 95, in __call__\n",
      "    result = self._call_fn(*args, **kwargs)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/wandb/apis/internal.py\", line 110, in execute\n",
      "    return self.client.execute(*args, **kwargs)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/gql/client.py\", line 52, in execute\n",
      "    result = self._get_result(document, *args, **kwargs)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/gql/client.py\", line 60, in _get_result\n",
      "    return self.transport.execute(document, *args, **kwargs)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/gql/transport/requests.py\", line 38, in execute\n",
      "    request = requests.post(self.url, **post_args)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/requests/api.py\", line 116, in post\n",
      "    return request('post', url, data=data, json=json, **kwargs)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/requests/api.py\", line 60, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/requests/sessions.py\", line 533, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/requests/sessions.py\", line 646, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/lam/anaconda3/envs/hugo/lib/python3.7/site-packages/requests/adapters.py\", line 498, in send\n",
      "    raise ConnectionError(err, request=request)\n",
      "requests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "wandb: Network error (ConnectionError), entering retry loop. See /home/lam/Desktop/hugo/kaggle_bengali_recognition/notebooks_demo/wandb/debug.log for full traceback.\n"
     ]
    }
   ],
   "source": [
    "if is_weighted_class_loss:\n",
    "    root_label = label_data[:, 0]\n",
    "    vowel_label = label_data[:, 1]\n",
    "    consonant_label = label_data[:, 2]\n",
    "\n",
    "    class_weight = \"balanced\"\n",
    "\n",
    "    root_cls_weight = compute_class_weight(class_weight, np.unique(root_label), root_label)\n",
    "    vowel_cls_weight = compute_class_weight(class_weight, np.unique(vowel_label), vowel_label)\n",
    "    consonant_cls_weight = compute_class_weight(class_weight, np.unique(consonant_label), consonant_label)\n",
    "    \n",
    "    class_weights = [\n",
    "        torch.Tensor(root_cls_weight).cuda(),\n",
    "        torch.Tensor(vowel_cls_weight).cuda(),\n",
    "        torch.Tensor(consonant_cls_weight).cuda(),\n",
    "    ]\n",
    "else:\n",
    "    class_weights = None\n",
    "    \n",
    "for i, (train_idx, valid_idx) in enumerate(zip(train_idx_list, test_idx_list)):\n",
    "    # skip unwanted fold\n",
    "    if i not in [0]:\n",
    "        continue\n",
    "        \n",
    "    print(f\"Training fold {i}\") \n",
    "        \n",
    "    # create model \n",
    "    model = model_arch(**model_parameter)\n",
    "    \n",
    "    # create optimizer\n",
    "    optimizer_ft = optimizer(model.parameters(), **optimizer_parameter)\n",
    "    \n",
    "    if swa:\n",
    "        optimizer_ft = torchcontrib.optim.SWA(optimizer_ft)\n",
    "        \n",
    "    if mixed_precision:\n",
    "        model.to(\"cuda\")\n",
    "        model, optimizer_ft = amp.initialize(model, optimizer_ft, opt_level=\"O1\")\n",
    "        model = nn.parallel.DataParallel(model)\n",
    "    else:\n",
    "        model.to(\"cuda\")\n",
    "        model = nn.DataParallel(model)\n",
    "\n",
    "    # create data loader\n",
    "    data_loaders = create_dataloaders_train(\n",
    "        image_data, name_data, label_data, train_idx, valid_idx, \n",
    "        data_transforms, batch_size, num_workers, pin_memory\n",
    "    )\n",
    "    \n",
    "    # create epoch_scheduler\n",
    "    if epoch_scheduler_func:\n",
    "        epoch_scheduler = epoch_scheduler_func(optimizer_ft, **epoch_scheduler_func_para)\n",
    "    else:\n",
    "        epoch_scheduler = None\n",
    "        \n",
    "    # create error_plateaus_scheduler\n",
    "    if error_plateau_scheduler_func:\n",
    "        error_plateau_scheduler = error_plateau_scheduler_func(optimizer_ft, **error_plateau_scheduler_func_para)\n",
    "    else:\n",
    "        error_plateau_scheduler = None\n",
    "        \n",
    "    # callbacks = {}\n",
    "    if save_dir:\n",
    "        full_save_dir = os.path.join(save_dir, f\"baseline_fold_{i}\")\n",
    "    else:\n",
    "        full_save_dir = None\n",
    "\n",
    "    wandb.init(name='baseline-single-pickle', project='my-project')\n",
    "        \n",
    "    train_input_args = {\n",
    "        \"model\": model, \n",
    "        \"optimizer\": optimizer_ft,\n",
    "        \"dataloaders\": data_loaders,\n",
    "        \"mixed_precision\": mixed_precision, \n",
    "        \"train_loss_prob\": train_loss_prob,\n",
    "        \"class_weights\": class_weights,\n",
    "        \"head_weights\": head_weights,\n",
    "        \"mixup_alpha\": mixup_alpha, \n",
    "        \"num_epochs\": n_epoch,\n",
    "        \"epoch_scheduler\": epoch_scheduler, \n",
    "        \"error_plateau_scheduler\": error_plateau_scheduler,\n",
    "        \"save_dir\": full_save_dir,\n",
    "        \"wandb_log\": wandb_log,\n",
    "        \"swa\": swa,\n",
    "    }\n",
    "        \n",
    "    callbacks = train_model(**train_input_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
