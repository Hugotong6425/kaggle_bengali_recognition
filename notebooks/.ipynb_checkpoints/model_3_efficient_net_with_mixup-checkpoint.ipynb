{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:15:06.169089Z",
     "start_time": "2020-01-25T13:15:06.149870Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:15:40.844704Z",
     "start_time": "2020-01-25T13:15:40.815861Z"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from hw_grapheme.train_mixup import generate_stratified_k_fold_index, train_model\n",
    "from hw_grapheme.utils import load_model_weight\n",
    "from hw_grapheme.data_pipeline import create_dataloaders, load_data\n",
    "from hw_grapheme.model import EfficientNet_grapheme, EfficientNet_0\n",
    "from hw_grapheme.loss_func import Loss_combine\n",
    "\n",
    "from torchtools.optim import RangerLars, RAdam\n",
    "# from one_cycle import OneCycleLR\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "# from warmup_scheduler import GradualWarmupScheduler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:15:07.000525Z",
     "start_time": "2020-01-25T13:15:06.985814Z"
    }
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "#     mean = np.array([0.485, 0.456, 0.406])\n",
    "#     std = np.array([0.229, 0.224, 0.225])\n",
    "#     inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:15:17.349456Z",
     "start_time": "2020-01-25T13:15:07.027789Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data done, shape: (200840, 224, 224), (200840,), (200840, 3)\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "pickle_paths = [\n",
    "    \"../data/processed_data/size_224/train_data_0.pickle\",\n",
    "    \"../data/processed_data/size_224/train_data_1.pickle\",\n",
    "    \"../data/processed_data/size_224/train_data_2.pickle\",\n",
    "    \"../data/processed_data/size_224/train_data_3.pickle\",\n",
    "]\n",
    "\n",
    "image_data, name_data, label_data = load_data(pickle_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:15:22.933462Z",
     "start_time": "2020-01-25T13:15:21.615151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_workers = 6\n",
    "pin_memory = True\n",
    "n_epoch = 120\n",
    "\n",
    "n_splits = 5\n",
    "random_seed = 2020\n",
    "\n",
    "mixed_precision = False\n",
    "\n",
    "train_idx_list, test_idx_list = generate_stratified_k_fold_index(\n",
    "    image_data, label_data, n_splits, random_seed\n",
    ")\n",
    "\n",
    "# create loss function\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = Loss_combine()\n",
    "\n",
    "# for discriminative lr\n",
    "# my_list = ['module._fc.weight', 'module._fc.bias']\n",
    "# params = list(filter(lambda kv: kv[0] in my_list, eff_b0.named_parameters()))\n",
    "# base_params = list(filter(lambda kv: kv[0] not in my_list, eff_b0.named_parameters()))\n",
    "# params = [kv[1] for kv in params]\n",
    "# base_params = [kv[1] for kv in base_params]\n",
    "\n",
    "# create data_transforms\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        # transforms.RandomAffine(degrees=10, scale=(1.0, 1.15)),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.0692], [0.2051]),\n",
    "        # transforms.ToPILImage(),\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.ToTensor(),\n",
    "        # transforms.Normalize([0.0692], [0.2051])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T12:28:13.727957Z",
     "start_time": "2020-01-25T12:28:13.552179Z"
    }
   },
   "source": [
    "pretrain = False\n",
    "eff_version = \"4\"\n",
    "\n",
    "eff_b0 = EfficientNet_grapheme(eff_version, pretrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T12:28:13.920229Z",
     "start_time": "2020-01-25T12:28:13.889987Z"
    }
   },
   "source": [
    "eff_b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-25T13:12:38.327116Z",
     "start_time": "2020-01-25T12:28:55.708412Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n",
      "Creating train dataloader...\n",
      "Creating test dataloader...\n",
      "Epoch 0/119\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8fb8cecf3f421e8f9a6fc7015676ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5021.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Loss: 5.6050, root_acc: 0.1689, vowel_acc: 0.5009, consonant_acc: 0.6812, combined_acc: 0.3800\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82f083180dec45038fc0b1802862d0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1256.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Loss: 0.8445, root_acc: 0.7500, vowel_acc: 0.9308, consonant_acc: 0.9326, combined_acc: 0.8409\n",
      "In epoch 0, highest val accuracy increases from 0.0 to 0.8408683529177454.\n",
      "In epoch 0, lowest val loss decreases from 999 to 0.8445002150901213.\n",
      "\n",
      "Epoch 1/119\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8144467fac034d338ca9002bf75dc555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5021.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _DataLoaderIter.__del__ at 0x7f44c5bd54d0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hugo/anaconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 677, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/hugo/anaconda3/envs/kaggle/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 659, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/hugo/anaconda3/envs/kaggle/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/hugo/anaconda3/envs/kaggle/lib/python3.7/multiprocessing/popen_fork.py\", line 48, in wait\n",
      "    return self.poll(os.WNOHANG if timeout == 0.0 else 0)\n",
      "  File \"/home/hugo/anaconda3/envs/kaggle/lib/python3.7/multiprocessing/popen_fork.py\", line 28, in poll\n",
      "    pid, sts = os.waitpid(self.pid, flag)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-257d29eddcda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0meff_b0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmixed_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mepoch_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"../model_weights/eff_6_with_mixup_cutmix/fold_{i}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     )\n",
      "\u001b[0;32m~/Desktop/kaggle/hw_grapheme/hw_grapheme/train_mixup.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, dataloaders, mixed_precision, callbacks, num_epochs, epoch_scheduler, batch_scheduler, save_dir)\u001b[0m\n\u001b[1;32m    260\u001b[0m         train_loss, train_root_acc, train_vowel_acc, train_consonant_acc = train_phrase(\n\u001b[1;32m    261\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mnum_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmixed_precision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         )\n\u001b[1;32m    264\u001b[0m         \u001b[0mcombined_train_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_root_acc\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_vowel_acc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_consonant_acc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/kaggle/hw_grapheme/hw_grapheme/train_mixup.py\u001b[0m in \u001b[0;36mtrain_phrase\u001b[0;34m(model, optimizer, train_dataloader, num_train, mixed_precision, batch_scheduler)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/kaggle/lib/python3.7/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pretrain = False\n",
    "# eff_version = \"4\"\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(zip(train_idx_list, test_idx_list)):\n",
    "    if i != 0:\n",
    "        continue\n",
    "    print(f\"Training fold {i}\")\n",
    "    \n",
    "    # create model \n",
    "    # eff_b0 = EfficientNet_grapheme(eff_version, pretrain)    \n",
    "    eff_b0 = EfficientNet_0(pretrain)\n",
    "\n",
    "    #########################\n",
    "    # load_model_weight(eff_b0, \"../model_weights/eff_0_with_mixup_cutmix/fold_0/eff_0_low_loss.pth\")\n",
    "    #########################\n",
    "    if mixed_precision:\n",
    "        eff_b0 = apex.parallel.DistributedDataParallel(eff_b0)\n",
    "        eff_b0.to(\"cuda\")\n",
    "        eff_b0 = torch.nn.parallel.DistributedDataParallel(\n",
    "            eff_b0, device_ids=[0,1], output_device=0\n",
    "        )\n",
    "        eff_b0, optimizer_ft = amp.initialize(eff_b0, optimizer_ft, opt_level=\"O1\")\n",
    "    else:\n",
    "        eff_b0.to(\"cuda\")\n",
    "        eff_b0 = nn.DataParallel(eff_b0)\n",
    "        \n",
    "    # create optimizer\n",
    "    # optimizer_ft = RangerLars(eff_b0.parameters())\n",
    "    optimizer_ft = optim.Adam(eff_b0.parameters(), weight_decay=1e-5)\n",
    "\n",
    "    # create data loader\n",
    "    data_loaders = create_dataloaders(\n",
    "        image_data, name_data, label_data, train_idx, valid_idx, \n",
    "        data_transforms, batch_size, num_workers, pin_memory\n",
    "    )\n",
    "    \n",
    "    # create lr scheduler\n",
    "    # exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(\n",
    "    #     optimizer_ft, factor=0.5, patience=5,\n",
    "    # )\n",
    "#     cos_lr_scheduler = lr_scheduler.CosineAnnealingLR(\n",
    "#         optimizer_ft, T_max=n_epoch,\n",
    "#     )\n",
    "#     # one_cycle_lr_scheduler = OneCycleLR(\n",
    "#     #     optimizer_ft, max_lr=0.01, steps_per_epoch=len(data_loaders[\"train\"]), epochs=n_epoch\n",
    "#     # )   \n",
    "    \n",
    "#     scheduler_warmup = GradualWarmupScheduler(\n",
    "#         optimizer_ft, multiplier=1, total_epoch=10, after_scheduler=cos_lr_scheduler\n",
    "#     )\n",
    "\n",
    "    \n",
    "    callbacks = {}\n",
    "\n",
    "    callbacks = train_model(\n",
    "        eff_b0, optimizer_ft, data_loaders,\n",
    "        mixed_precision, callbacks, num_epochs=n_epoch,\n",
    "        epoch_scheduler=None, save_dir=f\"../model_weights/eff_0_with_mixup_cutmix_alpha_0.1/fold_{i}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T17:54:26.204272Z",
     "start_time": "2020-01-23T17:54:26.180340Z"
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"model\": \"efficient 0\",\n",
    "    \"pretrain\": pretrain,\n",
    "    \"head_info\": \"1 fc\",\n",
    "    \"input_size\": \"224X224\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"n_fold\": n_splits,\n",
    "    \"split_seed\": random_seed,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epoch\": n_epoch,\n",
    "    \"mixed_precision\": mixed_precision,\n",
    "    \"image_processing\": \"mixup, cutmix\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-24T01:29:27.962758Z",
     "start_time": "2020-01-24T01:29:27.937492Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'efficient 0',\n",
       " 'pretrain': False,\n",
       " 'head_info': '1 fc',\n",
       " 'input_size': '224X224',\n",
       " 'optimizer': 'adam',\n",
       " 'n_fold': 5,\n",
       " 'split_seed': 2020,\n",
       " 'batch_size': 64,\n",
       " 'epoch': 64,\n",
       " 'mixed_precision': False,\n",
       " 'image_processing': 'mixup, cutmix'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_root_dir = \"../model_weights/eff_0_baseline\"\n",
    "\n",
    "\n",
    "config_save_path = os.path.join(save_root_dir, \"config.csv\")\n",
    "\n",
    "with open(config_save_path, \"w\") as f:\n",
    "    for key in configs.keys():\n",
    "        f.write(f\"{key},{configs[key]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-23T07:15:53.433438Z",
     "start_time": "2020-01-23T07:14:54.712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get a batch of training data for demo\n",
    "\n",
    "# visual_loader = DataLoader(\n",
    "#     train_dataset, batch_size=4,\n",
    "#     num_workers=num_workers, pin_memory=True,\n",
    "# )\n",
    "\n",
    "# inputs, a,b,c = next(iter(visual_loader))\n",
    "\n",
    "# # Make a grid from batch\n",
    "# out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "# imshow(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
